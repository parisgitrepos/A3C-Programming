{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from multiprocessing import Process, Queue\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class RL_Process(Process):\n",
    "    def __init__(self, *args, env: gym.Env = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recv_messages = Queue()\n",
    "        self.running = False\n",
    "        self.message_handlers = {'quit': self.quit}\n",
    "        self.env = env\n",
    "        \n",
    "    def create_networks(self):\n",
    "        import tensorflow as tf\n",
    "        self.tf = tf\n",
    "        self.actor = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "#             tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(self.env.action_space.n)\n",
    "        ])\n",
    "        self.critic = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "#             tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        obs = self.env.reset()\n",
    "        obs = np.expand_dims(obs, 0)\n",
    "        self.actor(obs)\n",
    "        self.critic(obs)\n",
    "        \n",
    "    def quit(self, message):\n",
    "        print(\"Received quit message\")\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        self.running = True\n",
    "        self.create_networks()\n",
    "        while self.running:\n",
    "            if not self.recv_messages.empty():\n",
    "                message = self.recv_messages.get()\n",
    "                message_id = message['id']\n",
    "                if message_id in self.message_handlers:\n",
    "                    self.message_handlers[message_id](message)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid message received: {message}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_process.recv_messages.put({'id': 'quit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma, standardized: bool = False):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(len(rewards))):                   \n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "    if standardized:\n",
    "        mean = np.mean(discounted_rewards)\n",
    "        discounted_rewards -= mean\n",
    "        standard_deviation = np.std(discounted_rewards)\n",
    "        discounted_rewards/=(standard_deviation + np.finfo(np.float32).eps) \n",
    "    return discounted_rewards\n",
    "\n",
    "class Worker_Process(RL_Process):\n",
    "    def __init__(self, *args, batch_size = 30, nn_process_queue = None, worker_id = 0, max_timesteps = 100, gamma = 0.95, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.nn_process_queue = nn_process_queue\n",
    "        self.worker_id = worker_id\n",
    "        self.message_handlers.update(train = self.train_batch) # same as next line\n",
    "        self.message_handlers['train'] = self.train_batch\n",
    "        self.message_handlers['weights'] = self.set_weights\n",
    "    \n",
    "    def send_message(self, message_id, **kwargs):\n",
    "        message = dict(id = message_id, **kwargs)\n",
    "        self.nn_process_queue.put(message)\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        logits = self.actor.predict(obs)\n",
    "        probablity_weights = self.tf.nn.softmax(logits = logits).numpy()[0]\n",
    "        action = np.random.choice(env.action_space.n, 1, p = probablity_weights)[0]\n",
    "        return action\n",
    "\n",
    "    def estimate_value(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        value = self.critic.predict(obs)\n",
    "        return value\n",
    "   \n",
    "    def critic_loss(self, observations, rewards):\n",
    "        huber_loss = self.tf.keras.losses.Huber(reduction=self.tf.keras.losses.Reduction.SUM)\n",
    "        values = self.critic(observations)\n",
    "        loss = huber_loss(values, rewards)\n",
    "        return loss\n",
    "\n",
    "    def actor_loss(self, actions, observations, values, rewards):\n",
    "        advantage = rewards - values\n",
    "        logits = self.actor(observations)\n",
    "        negative_log_prob = self.tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = actions)\n",
    "        loss = self.tf.reduce_mean(negative_log_prob*advantage)\n",
    "        return loss\n",
    "    \n",
    "    def run_episode(self):\n",
    "        obs = self.env.reset()\n",
    "        observations = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        for t in range(self.max_timesteps):\n",
    "            action = self.choose_action(obs)\n",
    "            value = self.estimate_value(obs)\n",
    "            observations.append(obs)\n",
    "            values.append(value)\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return observations, actions, values, rewards\n",
    "    \n",
    "    def run_batch(self, episodes, show_progress = False):\n",
    "        batch_observations = []\n",
    "        batch_values = []\n",
    "        batch_rewards = []\n",
    "        batch_actions = []\n",
    "        with tqdm(total = episodes, desc = 'Batch Progress', disable = not show_progress) as progress_bar:\n",
    "            for episode in range(episodes):\n",
    "                observations, actions, values, rewards = self.run_episode()\n",
    "                progress_bar.set_postfix_str(f'Episode Reward: {sum(rewards)}')\n",
    "                progress_bar.update()\n",
    "                batch_observations.extend(observations)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_values.extend(values)\n",
    "                rewards = discount_rewards(rewards, gamma = self.gamma, standardized = False)\n",
    "                batch_rewards.extend(rewards)\n",
    "        return batch_observations, batch_actions, batch_values, batch_rewards\n",
    "    \n",
    "    def train_step(self, observations, actions, values, rewards):\n",
    "        values = np.array(values)\n",
    "        rewards = np.array(rewards)\n",
    "        observations = np.array(observations)\n",
    "        # Step 1. Train actor using critic\n",
    "        with self.tf.GradientTape() as tape:\n",
    "            loss = self.actor_loss(actions, observations, values, rewards)\n",
    "            actor_gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    " \n",
    "        # Step 2. Train critic\n",
    "        with self.tf.GradientTape() as tape:\n",
    "            loss = self.critic_loss(observations, rewards)\n",
    "            critic_gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "        \n",
    "        return actor_gradients, critic_gradients\n",
    "    \n",
    "    def train_batch(self, message):\n",
    "        episode_assignment = message['episode_assignment']\n",
    "        mean_values = []\n",
    "        mean_rewards = []\n",
    "        episodes = []\n",
    "        for episode in range(episode_assignment):\n",
    "            observations, actions, values, rewards = self.run_batch(1, show_progress = False)\n",
    "            episodes.append(episode)\n",
    "            mean_values.append(np.mean(values))\n",
    "            mean_rewards.append(np.mean(rewards))\n",
    "            self.send_message('reward', reward = mean_rewards[-1], worker_id = self.worker_id, episode = episode)\n",
    "            actor_gradients, critic_gradients = self.train_step(observations, actions, values, rewards)\n",
    "            self.send_message('gradients', actor_gradients = actor_gradients, critic_gradients = critic_gradients)\n",
    "        self.send_message('complete', worker_id = self.worker_id, episodes_complete = episode_assignment)\n",
    "        \n",
    "    def set_weights(self, message):\n",
    "        self.actor.set_weights(message['actor_weights'])\n",
    "        self.critic.set_weights(message['critic_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class NN_Process(RL_Process):\n",
    "    def __init__(self, *args, max_workers = 4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_workers = max_workers\n",
    "        self.workers = [Worker_Process(nn_process_queue=self.recv_messages, worker_id=i, env = self.env) for i in range(max_workers)]\n",
    "        self.worker_rewards = {\n",
    "            worker.worker_id:dict(episodes = [], rewards = []) for worker in self.workers\n",
    "        }\n",
    "        self.message_handlers['gradients'] = self.apply_gradients\n",
    "        self.message_handlers['complete'] = self.worker_complete\n",
    "        self.message_handlers['start_training'] = self.setup_worker_assignments\n",
    "        self.message_handlers['reward'] = self.reward_update\n",
    "    \n",
    "    def reward_update(self, message):\n",
    "        self.worker_rewards[message['worker_id']]['rewards'].append(message['reward'])\n",
    "        self.worker_rewards[message['worker_id']]['episodes'].append(message['episode'])\n",
    "#       Use plt to create graph w/ diff worker rewards\n",
    "#       Loop over every item in worker_rewards and plot episodes(x) vs rewards(y)\n",
    "#       Add styles as needed\n",
    "    \n",
    "    def apply_gradients(self, message):\n",
    "        self.optimizer.apply_gradients(zip(message['actor_gradients'], self.actor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(message['critic_gradients'], self.critic.trainable_variables))\n",
    "        \n",
    "    def send_message(self, worker_id, message_id, **kwargs):\n",
    "        message = dict(id = message_id, **kwargs)\n",
    "        if worker_id < self.max_workers:\n",
    "            self.workers[worker_id].recv_messages.put(message)\n",
    "        else:\n",
    "            raise ValueError(f'Worker ID {worker_id} is invalid')\n",
    "        \n",
    "    def worker_complete(self, message):\n",
    "        worker_id = message['worker_id']\n",
    "        episodes_complete = message['episodes_complete']\n",
    "        self.pending_episodes -= episodes_complete\n",
    "        if self.total_episodes > 0:\n",
    "            self.assign_work(worker_id)\n",
    "        else:\n",
    "            self.send_message(worker_id, 'quit')\n",
    "            if self.pending_episodes <= 0:\n",
    "                self.running = False\n",
    "            \n",
    "    def start_training(self, total_episodes = 100, episodes_per_assignment = 50, max_steps_per_episode = 500):\n",
    "        for worker in self.workers:\n",
    "            if not worker.is_alive():\n",
    "                worker.start()\n",
    "        self.total_episodes = total_episodes\n",
    "        self.episodes_per_assignment = episodes_per_assignment\n",
    "        self.start()\n",
    "        self.recv_messages.put(dict(id = 'start_training', total_episodes = total_episodes, episodes_per_assignment = episodes_per_assignment, \n",
    "                                    max_steps_per_episode = max_steps_per_episode))\n",
    "        \n",
    "    def setup_worker_assignments(self, message):\n",
    "        self.total_episodes = message['total_episodes']\n",
    "        self.pending_episodes = 0\n",
    "        self.episodes_per_assignment = message['episodes_per_assignment']\n",
    "        for worker in self.workers:\n",
    "            self.assign_work(worker.worker_id)\n",
    "            \n",
    "    def assign_work(self, worker_id):\n",
    "        episode_assignment = min(self.episodes_per_assignment, self.total_episodes)\n",
    "        if episode_assignment > 0:\n",
    "            self.total_episodes -= episode_assignment\n",
    "            self.pending_episodes += episode_assignment\n",
    "            self.send_weights(worker_id)\n",
    "            self.send_message(worker_id, 'train', episode_assignment = episode_assignment)\n",
    "        \n",
    "    def send_weights(self, worker_id):\n",
    "        self.send_message(worker_id, 'weights', actor_weights = self.actor.get_weights(), \n",
    "                          critic_weights = self.critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Batch Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.77s/it, Episode Reward: 42.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.93s/it, Episode Reward: 42.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.84s/it, Episode Reward: 23.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.72s/it, Episode Reward: 23.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.04s/it, Episode Reward: 12.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:03<00:00,  3.97s/it, Episode Reward: 12.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.98s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.24s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.17s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.20s/it, Episode Reward: 13.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.18s/it, Episode Reward: 13.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.59s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.59s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.24s/it, Episode Reward: 50.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.30s/it, Episode Reward: 50.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it, Episode Reward: 25.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it, Episode Reward: 25.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.10s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.18s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.71s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it, Episode Reward: 21.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.47s/it, Episode Reward: 21.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.34s/it, Episode Reward: 19.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it, Episode Reward: 19.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.42s/it, Episode Reward: 41.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.54s/it, Episode Reward: 41.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.02s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.74s/it, Episode Reward: 21.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.77s/it, Episode Reward: 21.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.54s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.72s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.55s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.44s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.28s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.30s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.76s/it, Episode Reward: 29.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.95s/it, Episode Reward: 29.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.12s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.25s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:10<00:00, 10.89s/it, Episode Reward: 33.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:10<00:00, 10.66s/it, Episode Reward: 33.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.97s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.60s/it, Episode Reward: 25.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.56s/it, Episode Reward: 25.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.94s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.97s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.01s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.91s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.32s/it, Episode Reward: 23.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.38s/it, Episode Reward: 23.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.28s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:10<00:00, 10.87s/it, Episode Reward: 36.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:10<00:00, 10.92s/it, Episode Reward: 36.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.41s/it, Episode Reward: 45.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:13<00:00, 13.44s/it, Episode Reward: 45.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.40s/it, Episode Reward: 20.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.14s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.24s/it, Episode Reward: 26.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it, Episode Reward: 16.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it, Episode Reward: 27.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:08<00:00,  8.29s/it, Episode Reward: 27.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it, Episode Reward: 22.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.36s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:04<00:00,  4.52s/it, Episode Reward: 14.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.62s/it, Episode Reward: 17.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it, Episode Reward: 17.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:11<00:00, 11.53s/it, Episode Reward: 36.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:11<00:00, 11.36s/it, Episode Reward: 36.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.77s/it, Episode Reward: 19.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it, Episode Reward: 19.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:21<00:00, 21.24s/it, Episode Reward: 68.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:21<00:00, 21.50s/it, Episode Reward: 68.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.57s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it, Episode Reward: 24.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:14<00:00, 14.20s/it, Episode Reward: 45.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:14<00:00, 14.63s/it, Episode Reward: 45.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.59s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it, Episode Reward: 18.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.83s/it, Episode Reward: 50.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.83s/it, Episode Reward: 50.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.62s/it, Episode Reward: 53.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:15<00:00, 15.83s/it, Episode Reward: 53.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it, Episode Reward: 17.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Progress: 100%|██████████| 1/1 [00:05<00:00,  5.21s/it, Episode Reward: 17.0]\n",
      "Batch Progress: 100%|██████████| 1/1 [00:11<00:00, 11.37s/it, Episode Reward: 34.0]\n",
      "Process NN_Process-36:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-22-768a33c70636>\", line 42, in run\n",
      "    self.message_handlers[message_id](message)\n",
      "  File \"<ipython-input-45-b40aac30ac62>\", line 23, in worker_complete\n",
      "    episodes_complete = message['episodes_complete']\n",
      "KeyError: 'episodes_complete'\n",
      "Batch Progress: 100%|██████████| 1/1 [00:11<00:00, 11.58s/it, Episode Reward: 34.0]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "nn_manager = NN_Process(env = env, max_workers = 2)\n",
    "nn_manager.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
