{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class RL_Process(Process):\n",
    "    def __init__(self, *args, env: gym.Env = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recv_messages = Queue()\n",
    "        self.running = False\n",
    "        self.message_handlers = {'quit': self.quit}\n",
    "        self.env = env\n",
    "        \n",
    "    def create_networks(self):\n",
    "        import tensorflow as tf\n",
    "        self.tf = tf\n",
    "        self.actor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(env.action_space.n)\n",
    "        ])\n",
    "        self.critic = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def quit(self, message):\n",
    "        print(\"Received quit message\")\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        self.running = True\n",
    "        self.create_networks()\n",
    "        while self.running:\n",
    "            if not self.recv_messages.empty():\n",
    "                message = self.recv_messages.get()\n",
    "                message_id = message['id']\n",
    "                if message_id in self.message_handlers:\n",
    "                    self.message_handlers[message_id](message)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid message received: {message}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process RL_Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-2-de9aec5873dc>\", line 31, in run\n",
      "    self.create_networks()\n",
      "  File \"<ipython-input-2-de9aec5873dc>\", line 13, in create_networks\n",
      "    tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
      "AttributeError: 'NoneType' object has no attribute 'observation_space'\n"
     ]
    }
   ],
   "source": [
    "test_process = RL_Process()\n",
    "test_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_process.recv_messages.put({'id': 'quit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma, standardized: bool = False):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(len(rewards))):                   \n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "    if standardized:\n",
    "        mean = np.mean(discounted_rewards)\n",
    "        discounted_rewards -= mean\n",
    "        standard_deviation = np.std(discounted_rewards)\n",
    "        discounted_rewards/=(standard_deviation + np.finfo(np.float32).eps) \n",
    "    return discounted_rewards\n",
    "\n",
    "class Worker_Process(RL_Process):\n",
    "    def __init__(self, *args, batch_size = 30, nn_process_queue = None, worker_id = 0, max_timesteps = 100, gamma = 0.95, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.nn_process_queue = nn_process_queue\n",
    "        self.worker_id = worker_id\n",
    "        self.message_handlers.update(train = self.train_batch)\n",
    "        self.message_handlers['train'] = self.train_batch\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        logits = self.actor.predict(obs)\n",
    "        probablity_weights = tf.nn.softmax(logits = logits).numpy()[0]\n",
    "        action = np.random.choice(env.action_space.n, 1, p = probablity_weights)[0]\n",
    "        return action\n",
    "\n",
    "    def estimate_value(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        value = self.critic.predict(obs)\n",
    "        return value\n",
    "   \n",
    "    def critic_loss(self, observations, rewards):\n",
    "        huber_loss = self.tf.keras.losses.Huber(reduction=self.tf.keras.losses.Reduction.SUM)\n",
    "        values = self.critic(observations)\n",
    "        loss = huber_loss(values, rewards)\n",
    "        return loss\n",
    "\n",
    "    def actor_loss(self, actions, observations, values, rewards):\n",
    "        advantage = rewards - values\n",
    "        logits = self.actor(observations)\n",
    "        negative_log_prob = self.tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = actions)\n",
    "        loss = self.tf.reduce_mean(negative_log_prob*advantage)\n",
    "        return loss\n",
    "    \n",
    "    def run_episode(self):\n",
    "        obs = self.env.reset()\n",
    "        observations = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        for t in range(self.max_timesteps):\n",
    "            action = self.choose_action(obs)\n",
    "            value = self.estimate_value(obs)\n",
    "            observations.append(obs)\n",
    "            values.append(value)\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return observations, actions, values, rewards\n",
    "    \n",
    "    def run_batch(self, episodes):\n",
    "        batch_observations = []\n",
    "        batch_values = []\n",
    "        batch_rewards = []\n",
    "        batch_actions = []\n",
    "        with tqdm(total = episodes, desc = 'Batch Progress') as progress_bar:\n",
    "            for episode in range(episodes):\n",
    "                observations, actions, values, rewards = run_episode()\n",
    "                progress_bar.set_postfix_str(f'Episode Reward: {sum(rewards)}')\n",
    "                progress_bar.update()\n",
    "                batch_observations.extend(observations)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_values.extend(values)\n",
    "                rewards = discount_rewards(rewards, gamma = self.gamma, standardized = False)\n",
    "                batch_rewards.extend(rewards)\n",
    "        return batch_observations, batch_actions, batch_values, batch_rewards\n",
    "    \n",
    "    def train_step(self, observations, actions, values, rewards):\n",
    "        values = np.array(values)\n",
    "        rewards = np.array(rewards)\n",
    "        observations = np.array(observations)\n",
    "        # Step 1. Train actor using critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.actor_loss(actions, observations, values, rewards)\n",
    "            actor_gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    " \n",
    "        # Step 2. Train critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.critic_loss(observations, rewards)\n",
    "            critic_gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "        \n",
    "        return actor_gradients, critic_gradients\n",
    "    \n",
    "    def train_batch(self, message):\n",
    "        batches = message['batches']\n",
    "        mean_values = []\n",
    "        mean_rewards = []\n",
    "        episodes = []\n",
    "        for batch in range(batches):\n",
    "            observations, actions, values, rewards = self.run_batch(message['episodes'])\n",
    "            episodes.append(batch * self.batch_size)\n",
    "            mean_values.append(np.mean(values))\n",
    "            mean_rewards.append(np.mean(rewards))\n",
    "            actor_gradients, critic_gradients = self.train_step(observations, actions, values, rewards)\n",
    "            new_message = dict(id = 'gradients', actor_gradients = actor_gradients, critic_gradients = critic_gradients)\n",
    "            self.nn_process_queue.put(new_message)\n",
    "        self.nn_process_queue.put(dict(id = 'complete', worker_id = self.worker_id))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class NN_Process(RL_Process):\n",
    "    def __init__(self, *args, max_workers = 4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_workers = max_workers\n",
    "        self.workers = [Worker_Process(nn_process_queue=self.recv_messages, worker_id=i) for i in range(max_workers)]\n",
    "        self.message_handlers['gradients'] = self.apply_gradients\n",
    "        self.message_handlers['complete'] = self.worker_complete\n",
    "    def apply_gradients(self, message):\n",
    "        self.optimizer.apply_gradients(zip(message['actor_gradients'], self.actor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(message['critic_gradients'], self.critic.trainable_variables))\n",
    "        \n",
    "    def worker_complete(self, message):\n",
    "        worker_id = message['worker_id']\n",
    "        if worker_id < self.max_workers:\n",
    "            self.workers[worker_id].recv_messages.put(dict(id = 'quit'))\n",
    "        else:\n",
    "            raise ValueError(f'Worker ID {worker_id} is invalid')\n",
    "    def start_training(self, total_episodes = 100, batch_size = 50):\n",
    "        for worker in self.workers:\n",
    "            if not worker.is_alive():\n",
    "                worker.start()\n",
    "        self.total_episodes = total_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
