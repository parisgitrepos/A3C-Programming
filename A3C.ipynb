{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class RL_Process(Process):\n",
    "    def __init__(self, *args, env: gym.Env = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recv_messages = Queue()\n",
    "        self.running = False\n",
    "        self.message_handlers = {'quit': self.quit}\n",
    "        self.env = env\n",
    "        \n",
    "    def create_networks(self):\n",
    "        import tensorflow as tf\n",
    "        self.tf = tf\n",
    "        self.actor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(env.action_space.n)\n",
    "        ])\n",
    "        self.critic = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters = 16, kernel_size = 7, strides = 4, input_shape = self.env.observation_space.shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def quit(self, message):\n",
    "        print(\"Received quit message\")\n",
    "        self.running = False\n",
    "        \n",
    "    def run(self):\n",
    "        self.running = True\n",
    "        self.create_networks()\n",
    "        while self.running:\n",
    "            if not self.recv_messages.empty():\n",
    "                message = self.recv_messages.get()\n",
    "                message_id = message['id']\n",
    "                if message_id in self.message_handlers:\n",
    "                    self.message_handlers[message_id](message)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid message received: {message}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_process = RL_Process()\n",
    "test_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_process.recv_messages.put({'id': 'quit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma, standardized: bool = False):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(len(rewards))):                   \n",
    "        R = R * gamma + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "    if standardized:\n",
    "        mean = np.mean(discounted_rewards)\n",
    "        discounted_rewards -= mean\n",
    "        standard_deviation = np.std(discounted_rewards)\n",
    "        discounted_rewards/=(standard_deviation + np.finfo(np.float32).eps) \n",
    "    return discounted_rewards\n",
    "\n",
    "class Worker_Process(RL_Process):\n",
    "    def __init__(self, *args, batch_size = 30, nn_process_queue = None, worker_id = 0, max_timesteps = 100, gamma = 0.95, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.nn_process_queue = nn_process_queue\n",
    "        self.worker_id = worker_id\n",
    "        self.message_handlers.update(train = self.train_batch) # same as next line\n",
    "        self.message_handlers['train'] = self.train_batch\n",
    "        self.message_handlers['weights'] = self.set_weights\n",
    "    \n",
    "    def send_message(self, message_id, **kwargs):\n",
    "        message = dict(id = message_id, **kwargs)\n",
    "        self.nn_process_queue.put(message)\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        logits = self.actor.predict(obs)\n",
    "        probablity_weights = tf.nn.softmax(logits = logits).numpy()[0]\n",
    "        action = np.random.choice(env.action_space.n, 1, p = probablity_weights)[0]\n",
    "        return action\n",
    "\n",
    "    def estimate_value(self, obs):\n",
    "        obs = np.expand_dims(obs, axis = 0)\n",
    "        value = self.critic.predict(obs)\n",
    "        return value\n",
    "   \n",
    "    def critic_loss(self, observations, rewards):\n",
    "        huber_loss = self.tf.keras.losses.Huber(reduction=self.tf.keras.losses.Reduction.SUM)\n",
    "        values = self.critic(observations)\n",
    "        loss = huber_loss(values, rewards)\n",
    "        return loss\n",
    "\n",
    "    def actor_loss(self, actions, observations, values, rewards):\n",
    "        advantage = rewards - values\n",
    "        logits = self.actor(observations)\n",
    "        negative_log_prob = self.tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = actions)\n",
    "        loss = self.tf.reduce_mean(negative_log_prob*advantage)\n",
    "        return loss\n",
    "    \n",
    "    def run_episode(self):\n",
    "        obs = self.env.reset()\n",
    "        observations = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        for t in range(self.max_timesteps):\n",
    "            action = self.choose_action(obs)\n",
    "            value = self.estimate_value(obs)\n",
    "            observations.append(obs)\n",
    "            values.append(value)\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return observations, actions, values, rewards\n",
    "    \n",
    "    def run_batch(self, episodes):\n",
    "        batch_observations = []\n",
    "        batch_values = []\n",
    "        batch_rewards = []\n",
    "        batch_actions = []\n",
    "        with tqdm(total = episodes, desc = 'Batch Progress') as progress_bar:\n",
    "            for episode in range(episodes):\n",
    "                observations, actions, values, rewards = run_episode()\n",
    "                progress_bar.set_postfix_str(f'Episode Reward: {sum(rewards)}')\n",
    "                progress_bar.update()\n",
    "                batch_observations.extend(observations)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_values.extend(values)\n",
    "                rewards = discount_rewards(rewards, gamma = self.gamma, standardized = False)\n",
    "                batch_rewards.extend(rewards)\n",
    "        return batch_observations, batch_actions, batch_values, batch_rewards\n",
    "    \n",
    "    def train_step(self, observations, actions, values, rewards):\n",
    "        values = np.array(values)\n",
    "        rewards = np.array(rewards)\n",
    "        observations = np.array(observations)\n",
    "        # Step 1. Train actor using critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.actor_loss(actions, observations, values, rewards)\n",
    "            actor_gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    " \n",
    "        # Step 2. Train critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.critic_loss(observations, rewards)\n",
    "            critic_gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "        \n",
    "        return actor_gradients, critic_gradients\n",
    "    \n",
    "    def train_batch(self, message):\n",
    "        batches = message['batches']\n",
    "        mean_values = []\n",
    "        mean_rewards = []\n",
    "        episodes = []\n",
    "        for batch in range(batches):\n",
    "            observations, actions, values, rewards = self.run_batch(message['episodes'])\n",
    "            episodes.append(batch * self.batch_size)\n",
    "            mean_values.append(np.mean(values))\n",
    "            mean_rewards.append(np.mean(rewards))\n",
    "            actor_gradients, critic_gradients = self.train_step(observations, actions, values, rewards)\n",
    "            self.send_message('gradients', actor_gradients = actor_gradients, critic_gradients = critic_gradients)\n",
    "        self.send_message('complete', worker_id = self.worker_id)\n",
    "        \n",
    "    def set_weights(self, message):\n",
    "        self.actor.set_weights(message['actor_weights'])\n",
    "        self.critic.set_weights(message['critic_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class NN_Process(RL_Process):\n",
    "    def __init__(self, *args, max_workers = 4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_workers = max_workers\n",
    "        self.workers = [Worker_Process(nn_process_queue=self.recv_messages, worker_id=i) for i in range(max_workers)]\n",
    "        self.message_handlers['gradients'] = self.apply_gradients\n",
    "        self.message_handlers['complete'] = self.worker_complete\n",
    "    \n",
    "    def apply_gradients(self, message):\n",
    "        self.optimizer.apply_gradients(zip(message['actor_gradients'], self.actor.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(message['critic_gradients'], self.critic.trainable_variables))\n",
    "        \n",
    "    def send_message(self, worker_id, message_id, **kwargs):\n",
    "        message = dict(id = message_id, **kwargs)\n",
    "        if worker_id < self.max_workers:\n",
    "            self.workers[worker_id].recv_messages.put(message)\n",
    "        else:\n",
    "            raise ValueError(f'Worker ID {worker_id} is invalid')\n",
    "        \n",
    "    def worker_complete(self, message):\n",
    "        worker_id = message['worker_id']\n",
    "        self.send_message(worker_id, 'quit')\n",
    "    \n",
    "    def start_training(self, total_episodes = 100, batch_size = 50):\n",
    "        for worker in self.workers:\n",
    "            if not worker.is_alive():\n",
    "                worker.start()\n",
    "        self.total_episodes = total_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.start()\n",
    "        \n",
    "    def send_weights(self, worker_id):\n",
    "        self.send_message(worker_id, 'weights', actor_weights = self.actor.get_weights(), \n",
    "                          critic_weights = self.critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
